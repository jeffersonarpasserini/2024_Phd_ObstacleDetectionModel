import datetime
import os
import numpy as np
import pandas as pd
import scipy
import tensorflow as tf
import random

from matplotlib import pyplot as plt
from scipy.optimize import minimize_scalar
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import backend as K
from extract_features import load_data, modular_extract_features

# ConfiguraÃ§Ã£o de semente para reprodutibilidade
SEED = 1980
random.seed(SEED)
tf.random.set_seed(SEED)
np.random.seed(SEED)

# Definir caminhos
BASE_PATH = os.path.dirname(os.path.abspath(__file__))
RESULTS_PATH = os.path.join(BASE_PATH, 'results_details', 'loocv_results')
os.makedirs(RESULTS_PATH, exist_ok=True)


def build_model(input_shape, activation, dropout_rate, learning_rate, n_layers, n_neurons):
    model = Sequential()
    model.add(Input(shape=(input_shape,)))
    for _ in range(n_layers):
        model.add(Dense(n_neurons, activation=activation))
        if dropout_rate > 0:
            model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation='sigmoid'))
    optimizer = RMSprop(learning_rate=learning_rate)
    # model.compile(optimizer=optimizer, loss=BinaryCrossentropy(), metrics=['accuracy'])
    # CompilaÃ§Ã£o do modelo com a nova mÃ©trica F1
    model.compile(optimizer=optimizer, loss=f1_loss, metrics=['accuracy', f1_metric])

    return model

# executa teste de validaÃ§Ã£o cruzada
def run_kfold_cv(activation, dropout_rate, learning_rate, n_layers, n_neurons,
                 n_epochs, batch_size, early_stop_patience, lr_scheduler_patience,
                 model_type, n_splits=5):
    warnings = []
    history_log = {"epoch": [], "val_loss": [], "val_f1_score": []}
    df = load_data()

    labels = df["category"].replace({'clear': 1, 'non-clear': 0}).to_numpy().astype(int)
    features = modular_extract_features(df, model_type)

    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)
    results = []

    total_samples = len(df)
    print(f"Total de Amostras do Dataset {total_samples}...")

    for fold, (train_idx, test_idx) in enumerate(skf.split(features, labels), 1):
        print(f"ðŸ“Œ Rodando Fold {fold}/{n_splits}...")

        X_train, X_test = features[train_idx], features[test_idx]
        y_train, y_test = labels[train_idx], labels[test_idx]

        model = build_model(X_train.shape[1], activation, dropout_rate, learning_rate, n_layers, n_neurons)

        early_stopping = EarlyStopping(monitor='val_loss', patience=early_stop_patience, min_delta=0.001,
                                       restore_best_weights=True)
        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=lr_scheduler_patience, min_lr=1e-5)

        # Divide os dados de treino para criar um conjunto de validaÃ§Ã£o (80% treino / 20% validaÃ§Ã£o)
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=SEED)

        print(f"Amostras para Treinamento {len(X_train)}...")
        print(f"Amostras para ValidaÃ§Ã£o {len(X_val)}...")
        print(f"Amostras para Teste {len(X_test)}...")

        y_train = y_train.astype("float32")
        y_val = y_val.astype("float32")

        history = model.fit(X_train, y_train,
                            epochs=n_epochs,
                            batch_size=min(batch_size, len(X_train)),
                            validation_data=(X_val, y_val),
                            callbacks=[early_stopping, lr_scheduler],
                            verbose=0)

        # Salvando mÃ©tricas por Ã©poca
        for epoch, (vloss, vf1) in enumerate(zip(history.history["val_loss"], history.history["val_f1_metric"])):
            if epoch >= len(history_log["epoch"]):
                history_log["epoch"].append(epoch)
                history_log["val_loss"].append([])
                history_log["val_f1_score"].append([])

            history_log["val_loss"][epoch].append(vloss)
            history_log["val_f1_score"][epoch].append(vf1)

        y_pred_prob = model.predict(X_test).flatten()
        best_threshold = find_best_threshold(y_test, y_pred_prob)

        y_pred = (y_pred_prob > best_threshold).astype("int32")
        cm = confusion_matrix(y_test, y_pred)

        tn, fp, fn, tp = cm.flatten() if cm.shape == (2, 2) else (0, 0, 0, 0)

        if tp == 0:
            warnings.append(f"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Fold {fold} "
                            f"| Prob MÃ©dia: {y_pred_prob.mean():.4f} | Threshold: {best_threshold:.4f}")

        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else np.nan

        print(f"Fold {fold}/{n_splits} - Accuracy --> {accuracy}...")

        results.append({
            "Fold": fold,
            "Precision": precision_score(y_test, y_pred, zero_division=0),
            "Recall": recall_score(y_test, y_pred, zero_division=0),
            "F1-Score": f1_score(y_test, y_pred, zero_division=0),
            "Accuracy": accuracy,
            "ROC-AUC": roc_auc_score(y_test, y_pred_prob) if len(set(y_test)) > 1 else np.nan,
            "TN": tn, "FP": fp, "FN": fn, "TP": tp,
            "best_threshold": best_threshold
        })

    # MÃ©dia das mÃ©tricas de validaÃ§Ã£o ao longo das Ã©pocas
    mean_history_log = {
        "epoch": history_log["epoch"],
        "val_loss": [np.mean(epoch_values) for epoch_values in history_log["val_loss"]],
        "val_f1_score": [np.mean(epoch_values) for epoch_values in history_log["val_f1_score"]],
    }
    plot_mean_validation_metrics(mean_history_log)

    results_df = pd.DataFrame(results)
    results_df.to_csv(os.path.join(RESULTS_PATH, "kfold_results.csv"), index=False)
    print(f"ðŸ“Š Resultados K-Fold salvos em {RESULTS_PATH}")

    # Exibe mÃ©tricas mÃ©dias finais
    print("\nðŸ“Œ Resumo das mÃ©tricas (MÃ©dia entre os Folds):")
    print(results_df.mean(numeric_only=True))

    # Se houver warnings, salva-os
    if warnings:
        with open(os.path.join(RESULTS_PATH, "kfold_warnings.log"), "a") as log_file:
            log_file.write("\n".join(warnings) + "\n")

    # Gerar o resumo das mÃ©tricas finais
    analyze_results(results_df, "kfold")

    return results_df

# executa teste leave-one-out cross validation
def run_loocv(activation, dropout_rate, learning_rate, n_layers, n_neurons, n_epochs, batch_size, early_stop_patience,
              lr_scheduler_patience, model_type):

    warnings = []
    # Criar listas para armazenar os valores de loss e f1_score ao longo das Ã©pocas
    history_log = {"epoch": [], "val_loss": [], "val_f1_score": []}

    df = load_data()
    labels = df["category"].replace({'clear': 1, 'non-clear': 0}).to_numpy().astype(int)
    features = modular_extract_features(df, model_type)
    loo = LeaveOneOut()
    results = []
    total_samples = len(df)

    warnings_file = os.path.join(RESULTS_PATH, "warnings.log")
    if os.path.exists(warnings_file):
        os.remove(warnings_file)  # Remove arquivo antigo antes de iniciar nova rodada

    for i, (train_idx, test_idx) in enumerate(loo.split(features), 1):
        print(f"Processando imagem {i}/{total_samples}...")
        X_train, X_test = features[train_idx], features[test_idx]
        y_train, y_test = labels[train_idx], labels[test_idx]
        model = build_model(X_train.shape[1], activation, dropout_rate, learning_rate, n_layers, n_neurons)

        early_stopping = EarlyStopping(monitor='val_loss', patience=early_stop_patience, min_delta=0.001,
                                       restore_best_weights=True)
        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=lr_scheduler_patience, min_lr=1e-5)

        # Separar 80% para treino e 20% para validaÃ§Ã£o
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED)

        # Converter labels de validaÃ§Ã£o para float32
        y_train = y_train.astype("float32")
        y_val = y_val.astype("float32")

        # treina o modelo
        history = model.fit(X_train, y_train,
                      epochs=n_epochs,
                      batch_size=min(batch_size, len(X_train)),
                      validation_data=(X_val, y_val),
                      callbacks=[early_stopping, lr_scheduler],
                      verbose=0)

        #print("MÃ©tricas disponÃ­veis no history:", history.history.keys())

        # ðŸ“Œ Armazena os valores das mÃ©tricas ao longo das Ã©pocas
        for epoch, (vloss, vf1) in enumerate(zip(history.history["val_loss"], history.history["val_f1_metric"])):
            if epoch >= len(history_log["epoch"]):
                history_log["epoch"].append(epoch)
                history_log["val_loss"].append([])
                history_log["val_f1_score"].append([])

            history_log["val_loss"][epoch].append(vloss)
            history_log["val_f1_score"][epoch].append(vf1)

        # realiza a prediÃ§Ã£o
        y_pred_prob = model.predict(X_test)[0][0]

        # determina o melhor threshold para o conjunto.
        best_threshold = find_best_threshold(y_test, y_pred_prob)

        y_pred = (y_pred_prob > best_threshold).astype("int32")
        cm = confusion_matrix([y_test], [y_pred], labels=[0, 1])
        # CorreÃ§Ã£o para evitar erro se a matriz de confusÃ£o nÃ£o for 2x2
        tn, fp, fn, tp = cm.flatten() if cm.shape == (2, 2) else (0, 0, 0, 0)

        # ðŸš¨ VerificaÃ§Ã£o de TP = 0 (Nenhum Verdadeiro Positivo)
        if tp == 0:
            warnings.append(f"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - "
                            f"Arquivo: {df.iloc[test_idx]['filename'].values[0]} "
                            f"| Prob: {y_pred_prob:.4f} | Threshold: {best_threshold:.4f}")

        results.append({
            "filename": df.iloc[test_idx]["filename"].values[0],
            "y_true": y_test.item(),
            "y_pred": y_pred,
            "y_pred_prob": y_pred_prob,
            "best_threshold": best_threshold,
            "Precision": precision_score([y_test], [y_pred], zero_division=0),
            "Recall": recall_score([y_test], [y_pred], zero_division=0),
            "F1-Score": f1_score([y_test], [y_pred], zero_division=0),
            "TN": tn, "FP": fp, "FN": fn, "TP": tp
        })

    # Converte os valores armazenados para mÃ©dias por Ã©poca
    mean_history_log = {
        "epoch": history_log["epoch"],
        "val_loss": [np.mean(epoch_values) for epoch_values in history_log["val_loss"]],
        "val_f1_score": [np.mean(epoch_values) for epoch_values in history_log["val_f1_score"]],
    }
    plot_mean_validation_metrics(mean_history_log)

    mean_history_log_df = pd.DataFrame([mean_history_log])
    mean_history_log_df.to_csv(os.path.join(RESULTS_PATH, "mean_history_log.csv"), index=False)
    print(f"Resumo mean_history_log salvo em {RESULTS_PATH}")

    print("\nProcessamento concluÃ­do!")
    results_df = pd.DataFrame(results)
    results_df.to_csv(os.path.join(RESULTS_PATH, "loocv_results.csv"), index=False)
    print(f"Resultados LOOCV salvos em {RESULTS_PATH}")

    if warnings:
        with open(os.path.join(RESULTS_PATH, "warnings.log"), "a") as log_file:
            log_file.write("\n".join(warnings) + "\n")

    analyze_results(results_df)
    plot_threshold_distribution(results_df)

def find_best_threshold(y_test, y_pred_prob):
    def neg_f1(thresh):
        y_pred = (y_pred_prob > thresh).astype("int32")
        return -f1_score(np.array(y_test).ravel(), np.array(y_pred).ravel(), zero_division=0)  # <- ModificaÃ§Ã£o aqui

    result = minimize_scalar(neg_f1, bounds=(0.3, 0.6), method='bounded')
    return result.x if result.success else 0.6 # Threshold padrÃ£o: 0.5 --> ajustado para 0.6 testes anteriores com bons resultados

def calculate_median_threshold(df):
    thresholds = df["best_threshold"]
    Q1 = thresholds.quantile(0.25)
    Q3 = thresholds.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    filtered_thresholds = thresholds[(thresholds >= lower_bound) & (thresholds <= upper_bound)]
    median_threshold = filtered_thresholds.median()
    return median_threshold

def plot_threshold_distribution(df):
    plt.figure(figsize=(8, 6))  # Define um tamanho adequado para o grÃ¡fico
    plt.hist(df["best_threshold"], bins=20, edgecolor='black', alpha=0.7)
    plt.axvline(df["best_threshold"].median(), color='red', linestyle='dashed', label='Mediana')
    plt.xlabel("Threshold")
    plt.ylabel("FrequÃªncia")
    plt.title("DistribuiÃ§Ã£o dos Thresholds Otimizados")
    plt.legend()

    # Define o caminho onde o grÃ¡fico serÃ¡ salvo
    save_path = os.path.join(RESULTS_PATH, "threshold_distribution.png")

    # ðŸ”¹ Salva primeiro para garantir que a imagem Ã© escrita no arquivo antes de ser exibida
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ðŸ“Š GrÃ¡fico salvo em: {save_path}")

    # Agora sim, exibe o grÃ¡fico
    plt.show()

    # Fecha a figura para liberar memÃ³ria
    plt.close()

def analyze_results(df, test_type="loocv"):
    median_threshold = calculate_median_threshold(df)

    if test_type == "loocv":
        y_true = df["y_true"]
        y_pred = df["y_pred"]
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)

        if len(set(y_true)) > 1:
            roc_auc = roc_auc_score(y_true, df["y_pred_prob"])
        else:
            roc_auc = np.nan
            print("[AVISO] ROC-AUC nÃ£o pode ser calculado pois y_true contÃ©m apenas uma classe (tudo 0 ou tudo 1).")

        summary = {
            "Median Threshold (Filtered)": median_threshold,
            "Accuracy": (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else np.nan,
            "Precision": precision, "Recall": recall, "F1-Score": f1,
            "ROC-AUC": None if np.isnan(roc_auc) else roc_auc,
            "True Negatives": tn, "False Positives": fp, "False Negatives": fn, "True Positives": tp
        }
    else:
        summary = {
            "Median Threshold (Filtered)": median_threshold,
            "Accuracy": df["Accuracy"].mean(),
            "Precision": df["Precision"].mean(),
            "Recall": df["Recall"].mean(),
            "F1-Score": df["F1-Score"].mean(),
            "ROC-AUC": df["ROC-AUC"].mean(skipna=True),
            "True Negatives": df["TN"].mean(),
            "False Positives": df["FP"].mean(),
            "False Negatives": df["FN"].mean(),
            "True Positives": df["TP"].mean()
        }

    summary_df = pd.DataFrame([summary])

    summary_filename = "kfold_summary.csv" if "Fold" in df.columns else "loocv_summary.csv"
    summary_df.to_csv(os.path.join(RESULTS_PATH, summary_filename), index=False)

    print("Resumo Final das MÃ©tricas:")
    for key, value in summary.items():
        print(f"{key}: {value:.4f}")
    print(f"Resumo das mÃ©tricas salvo em {RESULTS_PATH}")

def f1_metric(y_true, y_pred):
    """ Calcula o F1-Score como mÃ©trica """
    y_true = K.cast(y_true, 'float32')  # ðŸ”¹ Converte para float antes da multiplicaÃ§Ã£o
    y_pred = K.round(y_pred)  # Converte probabilidades em classes binÃ¡rias (0 ou 1)

    tp = K.sum(y_true * y_pred)
    fp = K.sum((1 - y_true) * y_pred)
    fn = K.sum(y_true * (1 - y_pred))

    precision = tp / (tp + fp + K.epsilon())  # K.epsilon() evita divisÃ£o por zero
    recall = tp / (tp + fn + K.epsilon())

    return 2 * (precision * recall) / (precision + recall + K.epsilon())

# ----- Calulo da funÃ§Ã£o de perda no processo de aprendizado -------------------
def f1_loss(y_true, y_pred, alpha=0.6, beta=1):

    #alpha
    # Usa o parÃ¢metro alpha para controlar dinamicamente o peso entre F1 e precisÃ£o.
    # alpha = 0.6 --> resulta em 1 - (f1 * 0.6 + precision * 0.4)
    # 60% do peso f1 e 40% precisao

    #beta
    # Beta > 1 - favorece o recall (util se falsos negativos sao piores para o problema)
    # Beta < 1 - favorece a precisao (util se falsos positivos sao piores)
    # Beta = 1 - formula tradicional do F1-Score

    """ FunÃ§Ã£o de perda baseada no F1-Score """
    y_true = K.cast(y_true, 'float32')
    # y_pred = K.round(y_pred)  # Converte probabilidades para 0 ou 1

    tp = K.sum(y_true * y_pred)
    fp = K.sum((1 - y_true) * y_pred)
    fn = K.sum(y_true * (1 - y_pred))

    precision = tp / (tp + fp + K.epsilon())
    recall = tp / (tp + fn + K.epsilon())

    f1 = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + K.epsilon())

    return 1 - (alpha * f1 + (1 - alpha) * precision)


def plot_mean_validation_metrics(mean_history_log):
    plt.figure(figsize=(10, 6))

    # Plota a mÃ©dia de val_loss e val_f1_score ao longo das Ã©pocas
    plt.plot(mean_history_log["epoch"], mean_history_log["val_loss"], label="MÃ©dia Val Loss", color="blue", linestyle='-')
    plt.plot(mean_history_log["epoch"], mean_history_log["val_f1_score"], label="MÃ©dia Val F1-Score", color="red", linestyle='--')

    plt.xlabel("Ã‰poca")
    plt.ylabel("Valor MÃ©dio")
    plt.title("EvoluÃ§Ã£o MÃ©dia de Val Loss e Val F1-Score por Ã‰poca")
    plt.legend()

    # Salvar e exibir o grÃ¡fico
    save_path = os.path.join(RESULTS_PATH, "mean_val_loss_vs_val_f1_score.png")
    plt.savefig(save_path, dpi=300, bbox_inches="tight")
    print(f"ðŸ“Š GrÃ¡fico salvo em: {save_path}")
    plt.show()

if __name__ == "__main__":
    activation = 'relu'
    dropout_rate = 0.1
    learning_rate = 0.0001 # 0.0005 # 0.001
    n_layers = 1 #2
    n_neurons = 256 #512
    batch_size = 64
    n_epochs = 1000
    early_stop_patience = 150 #100 #50
    lr_scheduler_patience = 10
    model_type = 'MobileNetV1'
    n_splits = 10

    #Para rodar teste de leave-one-out cross validation descomente abaixo
    #run_loocv(activation, dropout_rate, learning_rate, n_layers, n_neurons, n_epochs, batch_size, early_stop_patience,
    #          lr_scheduler_patience, model_type)

    #Para rodar teste de validaÃ§Ã£o cruzada - descomente abaixo
    run_kfold_cv(activation, dropout_rate, learning_rate, n_layers, n_neurons, n_epochs, batch_size, early_stop_patience,
                lr_scheduler_patience, model_type, n_splits)
