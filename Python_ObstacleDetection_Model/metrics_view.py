from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

def calculate_metrics(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()

    accuracy = (tp + tn) / (tn + fp + fn + tp)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    roc = roc_auc_score(y_true, y_pred)

    return {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "Specificity": specificity,
        "F1-Score": f1,
        "ROC-AUC": roc
    }

def save_confusion_matrix(y_true, y_pred, result_path, filename="confusion_matrix.png"):
    cm = confusion_matrix(y_true, y_pred)
    os.makedirs(result_path, exist_ok=True)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False,
                xticklabels=['Positivo', 'Negativo'], yticklabels=['Positivo', 'Negativo'])
    plt.xlabel("Predito")
    plt.ylabel("Real")
    plt.title("Matriz de Confus√£o")
    plt.savefig(os.path.join(result_path, filename))
    plt.close()
